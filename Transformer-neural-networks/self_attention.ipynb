{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-0.27766923  0.30586403  0.01057463  0.36409474 -0.41489623  1.15277481\n",
      "   0.63715732 -1.05318505]\n",
      " [-0.6823625   0.72444694 -1.4419607  -0.50095249 -0.04408113  0.68822858\n",
      "   1.05971024 -0.10210544]\n",
      " [-0.74758765  0.58988275  0.2740787   0.33826729  1.34593399 -0.1037819\n",
      "   0.87886222 -0.59395793]\n",
      " [ 0.45122425 -0.41196376  1.01399581  0.53620411 -0.80208738  1.57122979\n",
      "  -0.97703709 -1.46250139]]\n",
      "K\n",
      " [[-0.4708874   1.64177464 -1.38780232  1.71701544  1.01333479  0.62804628\n",
      "  -0.54720871  0.94657378]\n",
      " [ 0.27079892  0.3888408   1.06496911  0.07948627 -1.4330064   0.13770432\n",
      "   0.31164302 -0.14493827]\n",
      " [ 1.11200785  1.39342085  0.45975428 -1.09075613  0.65111229 -0.76908524\n",
      "   0.17459603  1.11260402]\n",
      " [ 1.95817072  0.52294909  0.84582026 -0.61544628 -0.35686193 -0.79044713\n",
      "  -0.46193641 -1.23709805]]\n",
      "V\n",
      " [[ 0.89056773 -0.87976587 -0.44480487  0.96568199  1.63254008  1.2195493\n",
      "   0.71989661  0.77845321]\n",
      " [-0.90588885 -0.07982795 -1.01866175  1.84510543  0.13146088 -1.32809874\n",
      "  -0.40017139  1.07784997]\n",
      " [ 0.38218721 -2.68105677  1.04859167  0.70093381 -1.25073563  0.09439338\n",
      "  -2.07207328 -1.2197445 ]\n",
      " [ 1.18827287  1.06620003 -1.19065712  1.71442776 -0.25029707  0.96933714\n",
      "   0.83646095 -1.27176068]]\n"
     ]
    }
   ],
   "source": [
    "L, d_k, d_v = 4, 8, 8\n",
    "\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)\n",
    "\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention\n",
    "$$\n",
    "\\text{self attention} = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} + M \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new } V = \\text{self attention} \\cdot V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20138332,  1.18844554, -2.49210879, -0.35348884],\n",
       "       [ 2.36274522, -0.97556009, -0.35245351, -2.76014537],\n",
       "       [ 1.77648308, -1.23734722,  0.19645048, -1.20126238],\n",
       "       [-2.05108275,  2.3577449 , -3.71938331,  2.5006384 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6255806827997046, 0.8418219704856419, 3.549242551815472)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without the scaling factor, the self-attention mechanism would struggle to learn effectively because the gradients would either vanish or explode. By normalizing the dot products, the model can maintain a consistent gradient flow, making the training process more efficient and stable.\n",
    "\n",
    "##### The softmax function is sensitive to the scale of its input values. If the variance of the dot product is too high, the softmax function can become too steep, which means it will produce very small gradients for most inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6255806827997046, 0.8418219704856419, 0.443655318976934)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / np.sqrt(d_k)\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To prevent getting context from the future words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 0] = -np.Infinity\n",
    "mask[mask == 1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07119975,        -inf,        -inf,        -inf],\n",
       "       [ 0.83535658, -0.34491258,        -inf,        -inf],\n",
       "       [ 0.62808162, -0.4374683 ,  0.06945573,        -inf],\n",
       "       [-0.72516726,  0.8335887 , -1.31500058,  0.88410919]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.7649962 , 0.2350038 , 0.        , 0.        ],\n",
       "       [0.52177556, 0.17977168, 0.29845276, 0.        ],\n",
       "       [0.08844457, 0.4203686 , 0.04903541, 0.44215143]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "attention = softmax(scaled + mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.89056773, -0.87976587, -0.44480487,  0.96568199,  1.63254008,\n",
       "         1.2195493 ,  0.71989661,  0.77845321],\n",
       "       [ 0.4683936 , -0.69177741, -0.57966342,  1.17234985,  1.27978076,\n",
       "         0.62084232,  0.45667637,  0.84881258],\n",
       "       [ 0.41588814, -1.27355992, -0.10225977,  1.0447626 ,  0.50216696,\n",
       "         0.42574844, -0.31473101,  0.23590865],\n",
       "       [ 0.24209591,  0.22858748, -0.94258661,  1.65344097,  0.02765179,\n",
       "        -0.01720608,  0.1636889 , -0.10017723]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.89056773, -0.87976587, -0.44480487,  0.96568199,  1.63254008,\n",
       "         1.2195493 ,  0.71989661,  0.77845321],\n",
       "       [-0.90588885, -0.07982795, -1.01866175,  1.84510543,  0.13146088,\n",
       "        -1.32809874, -0.40017139,  1.07784997],\n",
       "       [ 0.38218721, -2.68105677,  1.04859167,  0.70093381, -1.25073563,\n",
       "         0.09439338, -2.07207328, -1.2197445 ],\n",
       "       [ 1.18827287,  1.06620003, -1.19065712,  1.71442776, -0.25029707,\n",
       "         0.96933714,  0.83646095, -1.27176068]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask = None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.89056773, -0.87976587, -0.44480487,  0.96568199,  1.63254008,\n",
       "          1.2195493 ,  0.71989661,  0.77845321],\n",
       "        [ 0.4683936 , -0.69177741, -0.57966342,  1.17234985,  1.27978076,\n",
       "          0.62084232,  0.45667637,  0.84881258],\n",
       "        [ 0.41588814, -1.27355992, -0.10225977,  1.0447626 ,  0.50216696,\n",
       "          0.42574844, -0.31473101,  0.23590865],\n",
       "        [ 0.24209591,  0.22858748, -0.94258661,  1.65344097,  0.02765179,\n",
       "         -0.01720608,  0.1636889 , -0.10017723]]),\n",
       " array([[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.7649962 , 0.2350038 , 0.        , 0.        ],\n",
       "        [0.52177556, 0.17977168, 0.29845276, 0.        ],\n",
       "        [0.08844457, 0.4203686 , 0.04903541, 0.44215143]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(q, k, v, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
