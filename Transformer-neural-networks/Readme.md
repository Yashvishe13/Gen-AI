# Transformers from Scratch

This repository aims to implement all the components used in Transformer models from scratch. The goal is to provide a comprehensive understanding of how Transformers work by building each part step-by-step without relying on high-level libraries.

## Table of Contents

- [Introduction](#introduction)
- [Components](#components)
- [Self-Attention](#self-attention)
- [Contributing](#contributing)
- [License](#license)

## Introduction

Transformer models have revolutionized the field of natural language processing (NLP) with their ability to process and generate human language efficiently. This repository breaks down the complexity of Transformers by implementing each of its components from scratch. It serves as an educational resource for those who wish to understand and build these models at a fundamental level.

## Components

### Self-Attention

The Self-Attention mechanism is a core component of Transformer models. It allows the model to weigh the importance of different words in a sentence when encoding a single word.

- **File:** `self_attention.ipynb`
- **Description:** This file contains an implementation of the self-attention mechanism. It includes the following key operations:
  - Scaled Dot-Product Attention
  - Multi-Head Attention

### Prerequisites

To run the code in this repository, you need to have Python installed along with the following libraries:

- numpy

You can install the required libraries using the following command:

```sh
pip install numpy

## Contributing
- [Yash Vishe]
